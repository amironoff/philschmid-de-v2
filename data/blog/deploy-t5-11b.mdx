---
title: 'Deploy T5 11B for inference for less than $500'
date: '2022-10-25'
lastmod: '2022-10-25'
tags:
  - HuggingFace
  - Transformers
  - Endpoints
  - bnb
draft: false
summary: Learn how to deploy T5 11B on a single GPU using Hugging Face Inference Endpoints.
images: ['/static/blog/deploy-t5-11b/thumbnail.png']
repository: https://huggingface.co/philschmid/t5-11b-sharded
---

This blog will teach you how to deploy [T5 11B](https://huggingface.co/t5-11b) for inference using [Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints). The T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) paper and is one of the most used and known Transformer models today.

T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 works well on various tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., for translation: _`translate English to German: ‚Ä¶`_, for summarization: _`summarize: ...`_

![t5.png](/static/blog/deploy-t5-11b/t5.png)

Before we can get started, make sure you meet all of the following requirements:

1. An Organization/User with an active plan and _WRITE_ access to the model repository.
2. You can access the UI: [https://ui.endpoints.huggingface.co](https://ui.endpoints.huggingface.co/endpoints)

The [Tutorial](https://file+.vscode-resource.vscode-cdn.net/Users/philipp/Projects/personal/blog/philschmid-de-v2/data/blog/deploy-t5-11b.md#tutorial-deploy-layoutlm-and-send-requests) will cover how to:

1. [Prepare model repository, custom handler, and additional dependencies](#1-prepare-model-repository-custom-handler-and-additional-dependencies)
2. [Deploy the custom handler as an Inference Endpoint](#2-deploy-the-custom-handler-as-an-inference-endpoint)
3. [Send HTTP request using Python](#3-send-http-request-using-python)

## What is Hugging Face Inference Endpoints?

[ü§ó Inference Endpoints](https://huggingface.co/inference-endpoints) offers a secure production solution to easily deploy Machine Learning models on dedicated and autoscaling infrastructure managed by Hugging Face.

A Hugging Face Inference Endpoint is built from a [Hugging Face Model Repository](https://huggingface.co/models). It supports all the [Transformers and Sentence-Transformers tasks](https://huggingface.co/docs/inference-endpoints/supported_tasks) and any arbitrary ML Framework through easy customization by adding a [custom inference handler.](https://huggingface.co/docs/inference-endpoints/guides/custom_handler) This [custom inference handler](https://huggingface.co/docs/inference-endpoints/guides/custom_handler) can be used to implement simple inference pipelines for ML Frameworks like Keras, Tensorflow, and scit-kit learn or can be used to add custom business logic to your existing transformers pipeline.

## Tutorial: Deploy T5-11B on a single NVIDIA T4

In this tutorial, you will learn how to deploy [T5 11B](https://huggingface.co/t5-11b) for inference using [Hugging Face Inference Endpoints](https://huggingface.co/inference-endpoints) and how you can integrate it via an API into your products.

## 1. Prepare model repository, custom handler, and additional dependencies

[T5 11B](https://huggingface.co/t5-11b) is, with 11 billion parameters of the largest openly available Transformer models. The weights in float32 are 45.2GB and are normally too big to deploy on an NVIDIA T4 with 16GB of GPU memory.

To be able to fit T5-11b into a single GPU, we are going to use two techniques:

- **mixed precision and sharding:** Converting the weights to fp16 will reduce the memory footprint by 2x, and sharding will allow us to easily place each ‚Äúshard‚Äù on a GPU without the need to load the model into CPU memory first.
- **LLM.int8():** introduces a new quantization technique for Int8 matrix multiplication, which cuts the memory needed for inference by half while. To learn more about check out this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) or the [paper](https://arxiv.org/abs/2208.07339).

We already prepared a repository with sharded fp16 weights of `T5-11B` on the Hugging Face Hub at: [philschmid/t5-11b-sharded](https://huggingface.co/philschmid/t5-11b-sharded). Those weights were created using the following snippet.

_Note: If you want to convert the weights yourself, e.g. to deploy [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl) you need at least 80GB of memory._

```python
import torch
from transformers import AutoModelWithLMHead
from huggingface_hub import HfApi

# load model as float16
model = AutoModelWithLMHead.from_pretrained("t5-11b", torch_dtype=torch.float16, low_cpu_mem_usage=True)
# shard model an push to hub
model.save_pretrained("sharded", max_shard_size="2000MB")
# push to hub
api = HfApi()
api.upload_folder(
    folder_path="sharded",
    repo_id="philschmid/t5-11b-sharded-fp16",
)
```

After we have our sharded fp16 model weights, we can prepare the additional dependencies we will need to use the \***\*LLM.int8().\*\*** LLM.int8() has been natively integrated into `transformers` through [bitsandbytes](https://github.com/TimDettmers/bitsandbytes).

To [add custom dependencies](https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies), we need to add a **`requirements.txt`** file to your model repository on the Hugging Face Hub with the Python dependencies you want to install.

```python
accelerate==0.13.2
bitsandbytes
```

The last step before creating our Inference Endpoint is to [create a custom Inference Handler](https://huggingface.co/docs/inference-endpoints/guides/custom_handler). If you want to learn how to create a custom Handler for Inference Endpoints, you can either checkout the [documentation](https://huggingface.co/docs/inference-endpoints/guides/custom_handler) or go through [‚ÄúCustom Inference with Hugging Face Inference Endpoints‚Äù](https://www.philschmid.de/custom-inference-handler).

```python
from typing import Dict, List, Any
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

class EndpointHandler:
    def __init__(self, path=""):
        # load model and processor from path
        self.model =  AutoModelForSeq2SeqLM.from_pretrained(path, device_map="auto", load_in_8bit=True)
        self.tokenizer = AutoTokenizer.from_pretrained(path)

    def __call__(self, data: Dict[str, Any]) -> Dict[str, str]:
        """
        Args:
            data (:obj:):
                includes the deserialized image file as PIL.Image
        """
        # process input
        inputs = data.pop("inputs", data)
        parameters = data.pop("parameters", None)

        # preprocess
        input_ids = self.tokenizer(inputs, return_tensors="pt").input_ids

        # pass inputs with all kwargs in data
        if parameters is not None:
            outputs = self.model.generate(input_ids, **parameters)
        else:
            outputs = self.model.generate(input_ids)

        # postprocess the prediction
        prediction = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        return [{"generated_text": prediction}]
```

## 2. Deploy the custom handler as an Inference Endpoint

UI: [https://ui.endpoints.huggingface.co/](https://ui.endpoints.huggingface.co/)

Since we prepared our model weights, dependencies and custom handler we can now deploy our model as an Inference Endpoint. We can deploy our custom Custom Handler the same way as a regular Inference Endpoint.

![model id](/static/blog/deploy-t5-11b/model.png)

Select the repository, the cloud, and the region. After that we need to open the ‚ÄúAdvanced Settings‚Äù to select `GPU ‚Ä¢ small ‚Ä¢ 1x NIVIDA Tesla T4` .

_Note: If you are trying to deploy the model on CPU the creation will fail_

![model id](/static/blog/deploy-t5-11b/instance.png)

The Inference Endpoint Service will check during the creation of your Endpoint if there is a `handler.py` available and will use it for serving requests no matter which ‚ÄúTask‚Äù you select.

The deployment can take 20-40 minutes due to the image artifact's model size (~30GB) build. After deploying our endpoint, we can test it using the inference widget.

![model id](/static/blog/deploy-t5-11b/inference.png)

## 3. Send HTTP request using Python

Hugging Face Inference endpoints can be used with an HTTP client in any language. We will use Python and the `requests` library to send our requests. (make your you have it installed `pip install requests`)

```python
import json
import requests as r

ENDPOINT_URL=""# url of your endpoint
HF_TOKEN=""

# payload samples
regular_payload = { "inputs": "translate English to German: The weather is nice today." }
parameter_payload = {
	"inputs": "translate English to German: Hello my name is Philipp and I am a Technical Leader at Hugging Face",
  "parameters" : {
    "max_length": 40,
  }
}

# HTTP headers for authorization
headers= {
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json"
}

# send request
response = r.post(ENDPOINT_URL, headers=headers, json=paramter_payload)
generated_text = response.json()

print(generated_text)
```

## Conclusion

That's it we successfully deploy our `T5-11b` to Hugging Face Inference Endpoints for less than $500.

To underline this again, we deployed one of the biggest available transformers in a managed, secure, scalable inference endpoint. This will allow Data scientists and Machine Learning Engineers to focus on R&D, improving the model rather than fiddling with MLOps topics.

Now, it's your turn! [Sign up](https://ui.endpoints.huggingface.co/new) and create your custom handler within a few minutes!

---

Thanks for reading! If you have any questions, feel free to contact me, through [Github](https://github.com/huggingface/transformers), or on the [forum](https://discuss.huggingface.co/c/optimum/59). You can also connect with me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).
